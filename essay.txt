1) You will observe that a large portion of the terms in the dictionary are numbers. However, we normally do not use numbers as query terms to search. Do you think it is a good idea to remove these number entries from the dictionary and the postings lists? Can you propose methods to normalize these numbers? How many percentage of reduction in disk storage do you observe after removing/normalizing these numbers?

Yes, since we do not use numbers as query terms to search, we do not need to store numerical terms in the dictionary. However, if the term is composed of a mix of letters and numbers, we should retain them. Some examples of such terms are iphone11 and 7up.
Without removing numerical values, the dictionary contains 32452 terms. In our current implementation, we discard numerical values and the count of terms in the dictionary is 20940. There is a reduction of 35%.

2) What do you think will happen if we remove stop words from the dictionary and postings file? How does it affect the searching phase?

Stop words account for 30%. It's removal will reduce the size of storage. However, in the event that the search query involves a stop word, then the system will not be able to retrieve the results for it.

3) The NLTK tokenizer may not correctly tokenize all terms. What do you observe from the resulting terms produced by sent_tokenize() and word_tokenize()? Can you propose rules to further refine these results?

sent_tokenize() is supposed to tokenise the given input into sentences. It seems that it identifies a new sentence by newline \n and full stop. The way of identifying a new sentence by a newline means that one sentence might be broken into two parts.

word_tokenize() is supposed to tokenise the given input into words. It is able to do it correctly for most of the time.
One thing to note is that for eg, "last year." becomes "last", "year" and "." where the punctuation mark is marked as a token.